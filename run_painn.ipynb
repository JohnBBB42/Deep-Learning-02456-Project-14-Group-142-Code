{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import mse_loss, gaussian_nll_loss\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "import copy\n",
    "import math\n",
    "from ase.db import connect\n",
    "from ase import Atoms\n",
    "from torch.utils.data import DataLoader\n",
    "from collections.abc import Sequence\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Implementations of Batch, reduce_splits, #\n",
    "# cosine_cutoff, BesselExpansion, compute_edge_vectors_and_norms, and sum_index\n",
    "###############################################################\n",
    "\n",
    "\"\"\"Data object classes and related utilities.\"\"\"\n",
    "\n",
    "###############################################################\n",
    "# BaseData, Data, AtomsData, GeometricData classes\n",
    "###############################################################\n",
    "\n",
    "class BaseData:\n",
    "    \"\"\"A dict-like base class for data objects.\n",
    "\n",
    "    Store all tensors in a dict for easy access and enumeration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.tensors = dict()\n",
    "        for key, value in kwargs.items():\n",
    "            self.__setattr__(key, value)\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        # try to get from self.tensors\n",
    "        if key in self.tensors:\n",
    "            return self.tensors[key]\n",
    "        # If not found in tensors, raise an AttributeError\n",
    "        raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{key}'\")\n",
    "\n",
    "    def __setattr__(self, key, value) -> None:\n",
    "        # store tensors in self.tensors and everything else in self.__dict__\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            self.tensors[key] = value\n",
    "            self.__dict__.pop(key, None) \n",
    "        else:\n",
    "            super().__setattr__(key, value)\n",
    "            self.tensors.pop(key, None)\n",
    "\n",
    "    def __getstate__(self) -> dict:\n",
    "        return self.__dict__\n",
    "\n",
    "    def __setstate__(self, state: dict) -> None:\n",
    "        self.__dict__ = state\n",
    "\n",
    "    def validate(self) -> bool:\n",
    "        for key, tensor in self.tensors.items():\n",
    "            assert isinstance(tensor, torch.Tensor), f\"'{key}' is not a tensor!\"\n",
    "        return True\n",
    "\n",
    "    def to(self, device: torch.device) -> None:\n",
    "        self.tensors = {k: v.to(device) for k, v in self.tensors.items()}\n",
    "\n",
    "\n",
    "\n",
    "class Data(BaseData):\n",
    "    \"\"\"A data object describing a homogeneous graph.\n",
    "\n",
    "    Includes general graph information about: nodes, edges, target labels and global features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_features: torch.Tensor,\n",
    "        edge_index: torch.Tensor = torch.tensor([]),\n",
    "        edge_features: torch.Tensor | None = None,\n",
    "        targets: torch.Tensor | None = None,\n",
    "        global_features: torch.Tensor | None = None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_features = node_features\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_features = edge_features\n",
    "        self.global_features = global_features\n",
    "        self.targets = targets\n",
    "\n",
    "    def validate(self) -> bool:\n",
    "        super().validate()\n",
    "        assert self.num_nodes > 0\n",
    "        assert self.node_features.shape[0] == self.num_nodes\n",
    "        assert self.node_features.ndim >= 2\n",
    "        assert self.edge_index.shape[0] == self.num_edges\n",
    "        assert self.edge_index.shape[0] == 0 or self.edge_index.shape[1] == 2\n",
    "        assert self.edge_index.shape[0] == 0 or self.edge_index.max() < self.num_nodes\n",
    "        if self.edge_features is not None:\n",
    "            assert self.edge_features.shape[0] == self.num_edges\n",
    "            assert self.num_edges == 0 or self.edge_features.ndim >= 2\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self) -> torch.Tensor:\n",
    "        # try to get num_nodes from tensors, else from node_features\n",
    "        return self.tensors.get(\"num_nodes\", self.node_features.shape[0])\n",
    "\n",
    "    @property\n",
    "    def num_edges(self) -> torch.Tensor:\n",
    "        # try to get num_edges from tensors, else from edge_index\n",
    "        return self.tensors.get(\"num_edges\", self.edge_index.shape[0])\n",
    "\n",
    "    @property\n",
    "    def edge_index_source(self) -> torch.Tensor:\n",
    "        return self.edge_index[:, 0]\n",
    "\n",
    "    @property\n",
    "    def edge_index_target(self) -> torch.Tensor:\n",
    "        return self.edge_index[:, 1]\n",
    "\n",
    "\n",
    "class AtomsData(Data):\n",
    "    \"\"\"A data object describing atoms as a graph with spatial information.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_positions: torch.Tensor,\n",
    "        energy: torch.Tensor | None = None,\n",
    "        forces: torch.Tensor | None = None,\n",
    "        magmoms: torch.Tensor | None = None,\n",
    "        cell: torch.Tensor | None = None,\n",
    "        volume: torch.Tensor | None = None,\n",
    "        stress: torch.Tensor | None = None,\n",
    "        pbc: torch.Tensor | None = None,\n",
    "        edge_shift: torch.Tensor | None = None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_positions = node_positions\n",
    "        self.energy = energy\n",
    "        self.forces = forces\n",
    "        self.magmoms = magmoms\n",
    "        self.cell = cell\n",
    "        self.volume = volume\n",
    "        self.stress = stress\n",
    "        self.pbc = pbc\n",
    "        self.edge_shift = edge_shift\n",
    "\n",
    "    def validate(self) -> bool:\n",
    "        super().validate()\n",
    "        assert self.node_positions.shape[0] == self.num_nodes\n",
    "        assert self.node_positions.ndim == 2\n",
    "        spatial_dim = self.node_positions.shape[1]\n",
    "        if self.energy is not None:\n",
    "            assert self.energy.shape == (1,)\n",
    "        if self.forces is not None:\n",
    "            assert self.forces.shape == (self.num_nodes, spatial_dim)\n",
    "        if self.magmoms is not None:\n",
    "            assert self.magmoms.shape == (self.num_nodes, 1)\n",
    "        if self.cell is not None or self.pbc is not None:\n",
    "            assert self.cell is not None\n",
    "            assert self.pbc is not None\n",
    "            assert self.cell.shape == (spatial_dim, spatial_dim)\n",
    "            assert self.pbc.shape == (spatial_dim,)\n",
    "        if self.volume is not None:\n",
    "            assert self.cell is not None\n",
    "            assert self.volume.shape == (1,)\n",
    "            assert torch.isclose(self.volume, torch.linalg.det(self.cell))\n",
    "        if self.stress is not None:\n",
    "            assert self.cell is not None\n",
    "            assert self.stress.shape == (spatial_dim, spatial_dim)\n",
    "        if self.edge_shift is not None:\n",
    "            assert self.edge_index is not None\n",
    "            assert self.edge_shift.shape == (self.num_edges, spatial_dim)\n",
    "        return True\n",
    "\n",
    "    def any_pbc(self) -> bool:\n",
    "        return self.pbc is not None and bool(torch.any(self.pbc))\n",
    "\n",
    "\n",
    "class GeometricData(Data):\n",
    "    \"\"\"A data object describing a geometric graph with spatial information.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_positions: torch.Tensor,\n",
    "        node_velocities: torch.Tensor | None = None,\n",
    "        node_accelerations: torch.Tensor | None = None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_positions = node_positions\n",
    "        self.node_velocities = node_velocities\n",
    "        self.node_accelerations = node_accelerations\n",
    "\n",
    "    def validate(self) -> bool:\n",
    "        super().validate()\n",
    "        assert self.node_positions.shape[0] == self.num_nodes\n",
    "        assert self.node_positions.ndim == 2\n",
    "        spatial_dim = self.node_positions.shape[1]\n",
    "        if self.node_velocities is not None:\n",
    "            assert self.node_velocities.shape[0] == self.num_nodes\n",
    "            assert self.node_velocities.shape[1] == spatial_dim\n",
    "        if self.node_accelerations is not None:\n",
    "            assert self.node_accelerations.shape[0] == self.num_nodes\n",
    "            assert self.node_accelerations.shape[1] == spatial_dim\n",
    "        return True\n",
    "\n",
    "###############################################################\n",
    "# Batch, collate_data and Utility functions\n",
    "###############################################################\n",
    "\n",
    "class Batch(Data):\n",
    "    \"\"\"An object representing a batch of data.\n",
    "\n",
    "    Typically a disjoint union of graphs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self._node_data_index: torch.Tensor | None = None\n",
    "        self._edge_data_index: torch.Tensor | None = None\n",
    "\n",
    "    def validate(self) -> bool:\n",
    "        for key, tensor in self.tensors.items():\n",
    "            assert isinstance(tensor, torch.Tensor), f\"'{key}' is not a tensor!\"\n",
    "        assert self.node_features.shape[0] == torch.sum(self.num_nodes)\n",
    "        assert self.node_features.shape[0] == 0 or self.node_features.ndim >= 2\n",
    "        assert self.edge_index.shape[0] == torch.sum(self.num_edges)\n",
    "        assert self.edge_index.shape[0] == 0 or self.edge_index.shape[1] == 2\n",
    "        assert self.edge_index.shape[0] == 0 or self.edge_index.max() < self.node_features.shape[0]\n",
    "        assert self.num_data >= 1\n",
    "        if self.edge_features is not None:\n",
    "            assert self.edge_features.shape[0] == self.edge_index.shape[0]\n",
    "            assert self.edge_features.ndim >= 2\n",
    "        if self.global_features is not None:\n",
    "            assert self.global_features.shape[0] == self.num_data\n",
    "        if self.targets is not None:\n",
    "            assert self.targets.ndim >= 2\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def num_data(self) -> int:\n",
    "        # Number of graphs in the batch is the length of the num_nodes tensor.\n",
    "        return self.num_nodes.shape[0]\n",
    "\n",
    "    @property\n",
    "    def node_data_index(self) -> torch.Tensor:\n",
    "        if self._node_data_index is None:\n",
    "            self._node_data_index = torch.repeat_interleave(\n",
    "                torch.arange(self.num_nodes.shape[0], device=self.num_nodes.device), self.num_nodes)\n",
    "        return self._node_data_index\n",
    "\n",
    "    @property\n",
    "    def edge_data_index(self) -> torch.Tensor:\n",
    "        if self._edge_data_index is None:\n",
    "            self._edge_data_index = torch.repeat_interleave(\n",
    "                torch.arange(self.num_edges.shape[0], device=self.num_edges.device), self.num_edges)\n",
    "        return self._edge_data_index\n",
    "\n",
    "\n",
    "def collate_data(list_of_data: Sequence[Data]) -> Batch:\n",
    "    \"\"\"Collate a list of data objects into a batch object.\n",
    "\n",
    "    The input graphs are combined into a single graph as a disjoint union by\n",
    "    concatenation of all data and appropriate adjustment of the edge_index.\n",
    "    \"\"\"\n",
    "    batch = dict()\n",
    "    batch[\"num_nodes\"] = torch.tensor([d.num_nodes for d in list_of_data])\n",
    "    batch[\"num_edges\"] = torch.tensor([d.num_edges for d in list_of_data])\n",
    "    offset = torch.cumsum(batch[\"num_nodes\"], dim=0) - batch[\"num_nodes\"]\n",
    "    batch[\"edge_index\"] = torch.cat([d.edge_index + offset[i] for i, d in enumerate(list_of_data)])\n",
    "    for k in list_of_data[0].tensors.keys():\n",
    "        if k not in batch.keys():\n",
    "            try:\n",
    "                if k == \"cell\" or k == \"stress\":\n",
    "                    batch[k] = torch.cat([d.tensors[k].unsqueeze(0) for d in list_of_data])\n",
    "                else:\n",
    "                    batch[k] = torch.cat([torch.atleast_2d(d.tensors[k]) for d in list_of_data])\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Failed to add '{k}' to batch:\", e)\n",
    "    return Batch(**batch)\n",
    "\n",
    "def sum_splits(values: torch.Tensor, splits: torch.Tensor, out: torch.Tensor | None = None) -> torch.Tensor:\n",
    "    if out is None:\n",
    "        out = torch.zeros((splits.size(0),) + values.shape[1:], dtype=values.dtype, device=values.device)\n",
    "    idx = torch.repeat_interleave(torch.arange(splits.size(0), device=values.device), splits)\n",
    "    out.index_add_(0, idx, values)\n",
    "    return out\n",
    "\n",
    "def mean_splits(values: torch.Tensor, splits: torch.Tensor, out: torch.Tensor | None = None) -> torch.Tensor:\n",
    "    out = sum_splits(values, splits, out=out)\n",
    "    # Divide by number of elements per split\n",
    "    divisor = splits.view(-1, *([1]*(values.dim()-1)))\n",
    "    out = out / divisor\n",
    "    return out\n",
    "\n",
    "def reduce_splits(values: torch.Tensor, splits: torch.Tensor, out: torch.Tensor | None = None, reduction: str = \"sum\") -> torch.Tensor:\n",
    "    if reduction == \"sum\":\n",
    "        return sum_splits(values, splits, out=out)\n",
    "    elif reduction == \"mean\":\n",
    "        return mean_splits(values, splits, out=out)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reduction method: {reduction}\")\n",
    "\n",
    "def cosine_cutoff(x: torch.Tensor, cutoff: float) -> torch.Tensor:\n",
    "    assert cutoff > 0.0\n",
    "    return 0.5 * (torch.cos(math.pi * x / cutoff) + 1) * (x <= cutoff)\n",
    "\n",
    "class BesselExpansion(torch.nn.Module):\n",
    "    def __init__(self, size: int, cutoff: float = 5.0, trainable: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.register_parameter(\n",
    "            \"b_pi_over_c\",\n",
    "            torch.nn.Parameter((torch.arange(size) + 1) * math.pi / cutoff, requires_grad=trainable)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + 1e-10\n",
    "        return torch.sin(self.b_pi_over_c * x) / x\n",
    "\n",
    "def compute_edge_vectors_and_norms(\n",
    "    positions: torch.Tensor,\n",
    "    edge_index: torch.Tensor,\n",
    "    edge_shift: torch.Tensor | None = None,\n",
    "    edge_cell: torch.Tensor | None = None,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    source_positions = positions[edge_index[:, 0]]\n",
    "    target_positions = positions[edge_index[:, 1]]\n",
    "    if edge_shift is not None:\n",
    "        assert edge_cell is not None\n",
    "        shift = torch.squeeze(edge_shift.unsqueeze(1) @ edge_cell, dim=1)\n",
    "        target_positions = target_positions + shift\n",
    "    vectors = target_positions - source_positions\n",
    "    norms = torch.linalg.norm(vectors, dim=1, keepdim=True)\n",
    "    return vectors, norms\n",
    "\n",
    "def sum_index(\n",
    "    values: torch.Tensor,\n",
    "    index: torch.Tensor,\n",
    "    out: torch.Tensor | None = None,\n",
    "    num_out: int = 0\n",
    ") -> torch.Tensor:\n",
    "    assert out is not None or num_out > 0\n",
    "    if out is None:\n",
    "        out_shape = torch.Size([num_out]) + values.shape[1:]\n",
    "        out = torch.zeros(out_shape, dtype=values.dtype, device=values.device)\n",
    "    out.index_add_(0, index, values)\n",
    "    return out\n",
    "\n",
    "###############################################################\n",
    "# Core PaiNN model code                                       #\n",
    "###############################################################\n",
    "\n",
    "\n",
    "class PaiNNInteractionBlock(nn.Module):\n",
    "    def __init__(self, node_size: int, edge_size: int, cutoff: float):\n",
    "        super().__init__()\n",
    "        self.node_size = node_size\n",
    "        self.edge_size = edge_size\n",
    "        self.cutoff = cutoff\n",
    "        self.edge_filter_net = nn.Linear(edge_size, 3 * node_size)\n",
    "        self.scalar_message_net = nn.Sequential(\n",
    "            nn.Linear(node_size, node_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(node_size, 3 * node_size),\n",
    "        )\n",
    "        self.U_net = nn.Linear(node_size, node_size, bias=False)\n",
    "        self.V_net = nn.Linear(node_size, node_size, bias=False)\n",
    "        self.a_net = nn.Sequential(\n",
    "            nn.Linear(2 * node_size, node_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(node_size, 3 * node_size),\n",
    "        )\n",
    "\n",
    "    def _message_function(self, node_states_scalar, node_states_vector, edge_states, edge_vectors, edge_norms, edge_index):\n",
    "        filter_weight = self.edge_filter_net(edge_states)\n",
    "        filter_weight = filter_weight * cosine_cutoff(edge_norms, self.cutoff)\n",
    "\n",
    "        scalar_output = self.scalar_message_net(node_states_scalar)\n",
    "        src_nodes = edge_index[:,0]\n",
    "        dst_nodes = edge_index[:,1]\n",
    "\n",
    "        filter_output = filter_weight * scalar_output[src_nodes]\n",
    "\n",
    "        gate_nodes, gate_edges, messages_scalar = torch.split(filter_output, self.node_size, dim=1)\n",
    "        gate_nodes = gate_nodes.unsqueeze(1)\n",
    "        gate_edges = gate_edges.unsqueeze(1)\n",
    "\n",
    "        gated_node_states_vector = node_states_vector[src_nodes]*gate_nodes\n",
    "        gated_edge_vectors = gate_edges * edge_vectors.unsqueeze(2)\n",
    "        messages_vector = gated_node_states_vector + gated_edge_vectors\n",
    "\n",
    "        delta_node_states_scalar_m = sum_index(messages_scalar, dst_nodes, torch.zeros_like(node_states_scalar))\n",
    "        delta_node_states_vector_m = sum_index(messages_vector, dst_nodes, torch.zeros_like(node_states_vector))\n",
    "\n",
    "        return delta_node_states_scalar_m, delta_node_states_vector_m\n",
    "\n",
    "    def _node_state_update_function(self, node_states_scalar, node_states_vector):\n",
    "        Uv = self.U_net(node_states_vector)\n",
    "        Vv = self.V_net(node_states_vector)\n",
    "        Vv_square_norm = torch.sum(Vv**2, dim=1)\n",
    "\n",
    "        a = self.a_net(torch.cat((node_states_scalar, Vv_square_norm), dim=1))\n",
    "        a_ss, a_sv, a_vv = torch.split(a, self.node_size, dim=1)\n",
    "\n",
    "        inner_prod_Uv_Vv = torch.sum(Uv*Vv, dim=1)\n",
    "        delta_node_states_scalar_u = a_ss + a_sv*inner_prod_Uv_Vv\n",
    "        delta_node_states_vector_u = a_vv.unsqueeze(1)*Uv\n",
    "        return delta_node_states_scalar_u, delta_node_states_vector_u\n",
    "\n",
    "    def forward(self, node_states_scalar, node_states_vector, edge_states, edge_vectors, edge_norms, edge_index):\n",
    "        delta_node_states_scalar_m, delta_node_states_vector_m = self._message_function(\n",
    "            node_states_scalar, node_states_vector, edge_states, edge_vectors, edge_norms, edge_index\n",
    "        )\n",
    "\n",
    "        node_states_scalar = node_states_scalar + delta_node_states_scalar_m\n",
    "        node_states_vector = node_states_vector + delta_node_states_vector_m\n",
    "\n",
    "        delta_node_states_scalar_u, delta_node_states_vector_u = self._node_state_update_function(\n",
    "            node_states_scalar, node_states_vector\n",
    "        )\n",
    "\n",
    "        node_states_scalar = node_states_scalar + delta_node_states_scalar_u\n",
    "        node_states_vector = node_states_vector + delta_node_states_vector_u\n",
    "        return node_states_scalar, node_states_vector\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    def __init__(self, node_size=64, edge_size=20, num_interaction_blocks=3, cutoff=5.0,\n",
    "                 pbc=False, use_readout=True, num_readout_layers=2, readout_size=1,\n",
    "                 readout_reduction=\"sum\"):\n",
    "        super().__init__()\n",
    "        self.node_size = node_size\n",
    "        self.edge_size = edge_size\n",
    "        self.num_interaction_blocks = num_interaction_blocks\n",
    "        self.cutoff = cutoff\n",
    "        self.pbc = pbc\n",
    "        self.use_readout = use_readout\n",
    "        self.num_readout_layers = num_readout_layers\n",
    "        self.readout_size = readout_size\n",
    "        self.readout_reduction = readout_reduction\n",
    "\n",
    "        num_embeddings = 119\n",
    "        self.node_embedding = nn.Embedding(num_embeddings, node_size)\n",
    "        self.edge_expansion = BesselExpansion(edge_size, cutoff)\n",
    "        self.interaction_blocks = nn.ModuleList(\n",
    "            PaiNNInteractionBlock(node_size, edge_size, cutoff)\n",
    "            for _ in range(num_interaction_blocks))\n",
    "\n",
    "        if self.use_readout:\n",
    "            layers = []\n",
    "            for _ in range(num_readout_layers - 1):\n",
    "                layers.append(nn.Linear(node_size, node_size))\n",
    "                layers.append(nn.SiLU())\n",
    "            layers.append(nn.Linear(node_size, readout_size))\n",
    "            self.readout_net = nn.Sequential(*layers)\n",
    "        else:\n",
    "            self.readout_net = None\n",
    "\n",
    "    def forward(self, batch):\n",
    "        node_states_scalar = self.node_embedding(batch.node_features.squeeze(-1))\n",
    "        node_states_vector = torch.zeros(\n",
    "            node_states_scalar.size(0), 3, self.node_size,\n",
    "            dtype=node_states_scalar.dtype,\n",
    "            device=node_states_scalar.device\n",
    "        )\n",
    "\n",
    "        # If we have multiple graphs batched, cell is (num_graphs, 3, 3).\n",
    "        # We need a (num_edges, 3, 3) cell_for_edges for each edge.\n",
    "        if (hasattr(batch, 'cell') and batch.cell is not None and\n",
    "            hasattr(batch, 'edge_shift') and batch.edge_shift is not None):\n",
    "            # Create a per-edge cell tensor\n",
    "            # batch.edge_data_index maps each edge to its graph index\n",
    "            cell_for_edges = batch.cell[batch.edge_data_index]  # shape: (total_edges, 3, 3)\n",
    "\n",
    "            edge_vectors, edge_norms = compute_edge_vectors_and_norms(\n",
    "                batch.node_positions, batch.edge_index,\n",
    "                batch.edge_shift, cell_for_edges\n",
    "            )\n",
    "        else:\n",
    "            # If we don't have multiple graphs or no cell/edge_shift, just pass them directly.\n",
    "            edge_vectors, edge_norms = compute_edge_vectors_and_norms(\n",
    "                batch.node_positions, batch.edge_index,\n",
    "                getattr(batch, 'edge_shift', None),\n",
    "                getattr(batch, 'cell', None)\n",
    "            )\n",
    "\n",
    "\n",
    "        edge_vectors = edge_vectors / (edge_norms+1e-10)\n",
    "        edge_states = self.edge_expansion(edge_norms)\n",
    "\n",
    "\n",
    "        for block in self.interaction_blocks:\n",
    "            node_states_scalar, node_states_vector = block(\n",
    "                node_states_scalar, node_states_vector,\n",
    "                edge_states, edge_vectors, edge_norms, batch.edge_index\n",
    "            )\n",
    "\n",
    "\n",
    "        if self.use_readout:\n",
    "            node_states_scalar = self.readout_net(node_states_scalar)\n",
    "\n",
    "        if self.readout_reduction:\n",
    "            output_scalar = reduce_splits(node_states_scalar, batch.num_nodes, reduction=self.readout_reduction)\n",
    "        else:\n",
    "            output_scalar = node_states_scalar\n",
    "\n",
    "        return output_scalar\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# Loss function and Training utilities #\n",
    "###############################################################\n",
    "\n",
    "class MSELoss(torch.nn.Module):\n",
    "    def __init__(self, target_property: str = \"\", forces: bool = False):\n",
    "        super().__init__()\n",
    "        self.target_property = target_property or \"energy\"\n",
    "        self.forces = forces\n",
    "\n",
    "    def forward(self, preds: dict, batch) -> torch.Tensor:\n",
    "        # Basic MSE on the target property\n",
    "        targets = batch.energy if self.target_property == \"energy\" else batch.targets\n",
    "        loss = mse_loss(preds[self.target_property], targets)\n",
    "        \n",
    "        # If forces are included, add them to the loss\n",
    "        if self.forces:\n",
    "            loss_forces = mse_loss(preds[\"forces\"], batch.forces)\n",
    "            # Combine equally for simplicity\n",
    "            loss = 0.5 * loss + 0.5 * loss_forces\n",
    "\n",
    "        return loss\n",
    "\n",
    "class GaussianNLLLoss(torch.nn.Module):\n",
    "    def __init__(self, target_property: str = \"\", variance: float = 1.0, forces: bool = False):\n",
    "        super().__init__()\n",
    "        self.target_property = target_property or \"energy\"\n",
    "        self.variance = variance\n",
    "        self.forces = forces\n",
    "\n",
    "    def forward(self, preds: dict, batch) -> torch.Tensor:\n",
    "        # Basic Gaussian NLL on the target property\n",
    "        targets = batch.energy if self.target_property == \"energy\" else batch.targets\n",
    "        var = torch.full_like(preds[self.target_property], self.variance)\n",
    "        loss = gaussian_nll_loss(preds[self.target_property], targets, var=var, reduction='mean')\n",
    "\n",
    "        # If forces are included, add them\n",
    "        if self.forces:\n",
    "            var_forces = torch.full_like(preds[\"forces\"], self.variance)\n",
    "            loss_forces = gaussian_nll_loss(preds[\"forces\"], batch.forces, var=var_forces, reduction='mean')\n",
    "            # Combine equally for simplicity\n",
    "            loss = 0.5 * loss + 0.5 * loss_forces\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Trainer with SWA, SWAG, SAM, ASAM and Laplace Implementation #\n",
    "################################################################\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, lr=1e-3, use_sam=False, use_asam=False, sam_rho=0.05,\n",
    "                 use_laplace=False, num_laplace_samples=10, prior_precision=0.0,\n",
    "                 use_swa=False, swa_lrs=1e-4, swa_start_percent=0.8, annealing_percent=0.05, \n",
    "                 annealing_strategy='cos', \n",
    "                 use_swag=False, max_num_models=20, no_cov_mat=True, loss_type=\"mse\", max_steps=10000): \n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.use_sam = use_sam\n",
    "        self.use_asam = use_asam\n",
    "        self.sam_rho = sam_rho\n",
    "\n",
    "        self.use_laplace = use_laplace\n",
    "        self.num_laplace_samples = num_laplace_samples\n",
    "        self.prior_precision = prior_precision\n",
    "\n",
    "        self.use_swa = use_swa\n",
    "        self.use_swag = use_swag\n",
    "        self.swa_lrs = swa_lrs\n",
    "        self.swa_start_percent = swa_start_percent\n",
    "        self.annealing_percent = annealing_percent\n",
    "        self.annealing_strategy = annealing_strategy\n",
    "        self.max_num_models = max_num_models\n",
    "        self.no_cov_mat = no_cov_mat\n",
    "\n",
    "        if self.use_sam or self.use_asam:\n",
    "            # Initialize base optimizer (SGD with Momentum)\n",
    "            self.optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "            mode = \"ASAM\" if self.use_asam else \"SAM\"\n",
    "            print(f\"Optimizer set to SGD with Momentum for {mode} (rho={self.sam_rho}.\")\n",
    "        else:\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            print(\"Optimizer set to Adam.\")\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.9999)\n",
    "\n",
    "        # Choose loss function\n",
    "        if loss_type == \"mse\":\n",
    "            print(\"Using MSE Loss.\")\n",
    "            self.loss_function = MSELoss(target_property=\"energy\", forces=False)\n",
    "        elif loss_type == \"nll\":\n",
    "            print(\"Using Gaussian NLL Loss.\")\n",
    "            self.loss_function = GaussianNLLLoss(target_property=\"energy\", variance=1.0, forces=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
    "\n",
    "        if use_swag:\n",
    "            self.num_parameters = sum(p.numel() for p in self.model.parameters())\n",
    "            device = next(self.model.parameters()).device  # Get device from model\n",
    "            self.mean = torch.zeros(self.num_parameters, device=device)\n",
    "            self.sq_mean = torch.zeros(self.num_parameters, device=device)\n",
    "            if not self.no_cov_mat:\n",
    "                self.cov_mat_sqrt = torch.zeros(self.max_num_models, self.num_parameters, device=device)\n",
    "            self.num_models_collected = 0\n",
    "\n",
    "        if use_laplace:\n",
    "            self._init_laplace()\n",
    "\n",
    "        if use_sam and use_asam:\n",
    "            raise ValueError(\"Cannot use both SAM and ASAM simultaneously.\")\n",
    "        \n",
    "        if self.use_swa and self.use_swag:\n",
    "            raise ValueError(\"Cannot use both SWA and SWAG simultaneously.\")\n",
    "\n",
    "        self.current_step = 0          # Initialize step counter\n",
    "        self.max_steps = max_steps      # Total number of training steps\n",
    "        self.swa_start_step = None      # Will be set in set_max_steps\n",
    "        self.annealing_steps = None     # Will be set in set_max_steps\n",
    "        self.epoch = 0  # Initialize epoch counter\n",
    "\n",
    "        # Initialize SWA/SWAG if enabled\n",
    "        if self.use_swa or self.use_swag:\n",
    "            self.setup_swa()\n",
    "\n",
    "    def set_max_steps(self, max_steps):\n",
    "        \"\"\"Set the maximum number of training steps and compute SWA parameters.\"\"\"\n",
    "        self.max_steps = max_steps\n",
    "        if self.use_swa or self.use_swag:\n",
    "            self.swa_start_step = int(self.swa_start_percent * self.max_steps)\n",
    "            self.annealing_steps = int(self.annealing_percent * self.max_steps)\n",
    "            print(f\"SWA will start at step {self.swa_start_step} and anneal over {self.annealing_steps} steps.\")\n",
    "\n",
    "    def setup_swa(self):\n",
    "        if self.use_swa:\n",
    "            self.averaged_model = AveragedModel(self.model)\n",
    "            self.swa_scheduler = SWALR(self.optimizer, swa_lr=self.swa_lrs)\n",
    "            print(\"SWA has been set up.\")\n",
    "        \n",
    "        if self.use_swag:\n",
    "            # SWAG setup already handled in __init__\n",
    "            print(\"SWAG has been set up.\")\n",
    "\n",
    "    def _init_laplace(self):\n",
    "        self.accumulated_squared_gradients = [torch.zeros_like(p) for p in self.model.parameters()]\n",
    "        self.total_batches = 0\n",
    "\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        self.model.train()\n",
    "        # Forward pass\n",
    "        pred = self.model(batch)\n",
    "        # Assume batch.energy is the target\n",
    "        preds_dict = {\"energy\": pred}  \n",
    "        loss = self.loss_function(preds_dict, batch)\n",
    "\n",
    "        if self.use_sam:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Compute gradient norm\n",
    "            grad_norm = torch.norm(\n",
    "                torch.stack([p.grad.detach().norm(2) for p in self.model.parameters() if p.grad is not None])\n",
    "            )\n",
    "\n",
    "            # SAM perturbation\n",
    "            e_ws = []\n",
    "            with torch.no_grad():\n",
    "                for p in self.model.parameters():\n",
    "                    if p.grad is None:\n",
    "                        e_ws.append(None)\n",
    "                        continue\n",
    "                    e_w = p.grad / (grad_norm + 1e-12) * self.sam_rho\n",
    "                    p.add_(e_w)\n",
    "                    e_ws.append(e_w)\n",
    "\n",
    "            # Second forward-backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            pred2 = self.model(batch)\n",
    "            loss2 = self.loss_function({\"energy\": pred2}, batch)  # Corrected line\n",
    "            loss2.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p, e_w in zip(self.model.parameters(), e_ws):\n",
    "                    if e_w is not None:\n",
    "                        p.sub_(e_w)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Define print frequency (e.g., every 10 steps)\n",
    "            print_frequency = 10\n",
    "            if self.current_step % print_frequency == 0:\n",
    "                # Calculate average perturbation norm\n",
    "                perturbation_norms = [e_w.norm().item() for e_w in e_ws if e_w is not None]\n",
    "                avg_perturbation_norm = (\n",
    "                    sum(perturbation_norms) / len(perturbation_norms) if perturbation_norms else 0.0\n",
    "                )\n",
    "\n",
    "                print(f\"[SAM] Step {self.current_step}, Gradient Norm: {grad_norm.item():.4f}\")\n",
    "                print(f\"[SAM] Step {self.current_step}, Average Perturbation Norm: {avg_perturbation_norm:.6f}\")\n",
    "                print(f\"[SAM] Step {self.current_step}, Loss Before SAM: {loss.item():.6f}, Loss After SAM: {loss2.item():.6f}\")\n",
    "                print(f\"[SAM] Step {self.current_step} SAM optimization completed.\")\n",
    "\n",
    "            final_loss = loss2\n",
    "\n",
    "        elif self.use_asam:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            param_norms = []\n",
    "            with torch.no_grad():\n",
    "                for p in self.model.parameters():\n",
    "                    if p.grad is None:\n",
    "                        param_norms.append(None)\n",
    "                        continue\n",
    "                    param_norm = torch.norm(p)\n",
    "                    param_norms.append(param_norm)\n",
    "                scaled_grads = []\n",
    "                for p, pn in zip(self.model.parameters(), param_norms):\n",
    "                    if p.grad is None:\n",
    "                        continue\n",
    "                    scaled_grad = p.grad / (pn + 1e-12)\n",
    "                    scaled_grads.append(scaled_grad.view(-1))\n",
    "                scaled_grad_norm = torch.norm(torch.cat(scaled_grads))\n",
    "                epsilon = self.sam_rho / (scaled_grad_norm + 1e-12)\n",
    "\n",
    "                perturbations = []\n",
    "                for p, pn in zip(self.model.parameters(), param_norms):\n",
    "                    if p.grad is None:\n",
    "                        perturbations.append(None)\n",
    "                        continue\n",
    "                    perturbation = epsilon * p.grad / (pn + 1e-12)\n",
    "                    p.add_(perturbation)\n",
    "                    perturbations.append(perturbation)\n",
    "\n",
    "            # Second forward-backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            pred2 = self.model(batch)\n",
    "            loss2 = self.loss_function({\"energy\": pred2}, batch)  # Corrected line\n",
    "            loss2.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p, perturbation in zip(self.model.parameters(), perturbations):\n",
    "                    if perturbation is not None:\n",
    "                        p.sub_(perturbation)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Define print frequency (e.g., every 10 steps)\n",
    "            print_frequency = 10\n",
    "            if self.current_step % print_frequency == 0:\n",
    "                # Calculate average perturbation norm for ASAM\n",
    "                perturbation_norms_asam = [perturbation.norm().item() for perturbation in perturbations if perturbation is not None]\n",
    "                avg_perturbation_norm_asam = (\n",
    "                    sum(perturbation_norms_asam) / len(perturbation_norms_asam) if perturbation_norms_asam else 0.0\n",
    "                )\n",
    "\n",
    "                print(f\"[ASAM] Step {self.current_step}, Average Perturbation Norm: {avg_perturbation_norm_asam:.6f}\")\n",
    "                print(f\"[ASAM] Step {self.current_step}, Loss Before ASAM: {loss.item():.6f}, Loss After ASAM: {loss2.item():.6f}\")\n",
    "                print(f\"[ASAM] Step {self.current_step} ASAM optimization completed.\")\n",
    "\n",
    "            final_loss = loss2\n",
    "            \n",
    "        else:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if self.use_laplace:\n",
    "                # Accumulate grad^2\n",
    "                with torch.no_grad():\n",
    "                    for i, p in enumerate(self.model.parameters()):\n",
    "                        if p.grad is not None:\n",
    "                            self.accumulated_squared_gradients[i] += p.grad.data.clone() ** 2\n",
    "                self.total_batches += 1\n",
    "            self.optimizer.step()\n",
    "            final_loss = loss\n",
    "\n",
    "        # Increment step counter\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Handle SWA based on steps\n",
    "        if self.use_swa and self.current_step >= self.swa_start_step:\n",
    "            # Update SWA parameters\n",
    "            self.averaged_model.update_parameters(self.model)\n",
    "            # Annealing the SWA learning rate\n",
    "            if self.annealing_steps > 0 and self.current_step <= (self.swa_start_step + self.annealing_steps):\n",
    "                self.swa_scheduler.step()\n",
    "            elif self.annealing_steps > 0 and self.current_step > (self.swa_start_step + self.annealing_steps):\n",
    "                # After annealing_steps, set SWA LR to a minimum value or keep it constant\n",
    "                pass  # You can implement a strategy here if needed\n",
    "\n",
    "        # Handle SWAG if enabled\n",
    "        if self.use_swag and self.current_step >= self.swa_start_step:\n",
    "            self.collect_swag_model()\n",
    "\n",
    "        return final_loss.item()\n",
    "\n",
    "    def end_epoch(self):\n",
    "        # Step the scheduler\n",
    "        self.scheduler.step()\n",
    "\n",
    "        # Increment epoch counter\n",
    "        self.epoch += 1\n",
    "\n",
    "    def collect_swag_model(self):\n",
    "        param_vector = torch.nn.utils.parameters_to_vector(self.model.parameters())\n",
    "        if torch.isnan(param_vector).any():\n",
    "            print(\"NaNs detected in parameter vector during SWAG collection!\")\n",
    "        \n",
    "        n = self.num_models_collected\n",
    "        if n == 0:\n",
    "            self.mean.copy_(param_vector)\n",
    "            self.sq_mean.copy_(param_vector**2)\n",
    "        else:\n",
    "            delta = param_vector - self.mean\n",
    "            self.mean += delta / (n + 1)\n",
    "            delta2 = param_vector**2 - self.sq_mean\n",
    "            self.sq_mean += delta2 / (n + 1)\n",
    "        if not self.no_cov_mat and n < self.max_num_models:\n",
    "            idx = n % self.max_num_models\n",
    "            self.cov_mat_sqrt[idx].copy_(param_vector - self.mean)\n",
    "        self.num_models_collected += 1\n",
    "\n",
    "    def swag_sample(self, scale=1.0, cov=False):\n",
    "        mean = self.mean\n",
    "        sq_mean = self.sq_mean\n",
    "        var = sq_mean - mean**2\n",
    "        \n",
    "        # **Clamp variance to zero to avoid negative values**\n",
    "        var = torch.clamp(var, min=0.0)\n",
    "        \n",
    "        std = torch.sqrt(var + 1e-30)\n",
    "        \n",
    "        if torch.isnan(mean).any() or torch.isnan(std).any():\n",
    "            print(\"NaNs detected in SWAG mean or std!\")\n",
    "        \n",
    "        z = torch.randn_like(mean)\n",
    "        if cov and not self.no_cov_mat:\n",
    "            c = self.cov_mat_sqrt[:min(self.num_models_collected, self.max_num_models)]\n",
    "            z_cov = torch.randn(c.size(0), device=c.device)\n",
    "            sample = mean + scale * (z * std + (c.t().matmul(z_cov) / (self.num_models_collected - 1)**0.5))\n",
    "        else:\n",
    "            sample = mean + scale * z * std\n",
    "        \n",
    "        if torch.isnan(sample).any():\n",
    "            print(\"NaNs detected in SWAG sampled parameters!\")\n",
    "        \n",
    "        torch.nn.utils.vector_to_parameters(sample, self.model.parameters())\n",
    "\n",
    "\n",
    "    def predict(self, batch):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if self.use_laplace and hasattr(self, 'hessian_diagonal'):\n",
    "                predictions = []\n",
    "                \n",
    "                # Clamp Hessian diagonals to avoid extreme values\n",
    "                for i, var_p in enumerate(self.hessian_diagonal):\n",
    "                    self.hessian_diagonal[i] = torch.clamp(var_p, min=1e-10)\n",
    "\n",
    "                # Sample parameters multiple times\n",
    "                for _ in range(self.num_laplace_samples):\n",
    "                    sampled_params = []\n",
    "                    for mean_p, var_p, p in zip(self.param_means, self.hessian_diagonal, self.model.parameters()):\n",
    "                        var_p = torch.clamp(var_p, min=1e-10)\n",
    "                        std = (1.0 / (var_p + 1e-6))**0.5\n",
    "                        std = torch.clamp(std, max=0.001)\n",
    "                        noise = torch.randn_like(std)\n",
    "                        sampled_p = mean_p + noise * std\n",
    "                        sampled_params.append(sampled_p)\n",
    "\n",
    "                    # Backup current params\n",
    "                    backup_params = [p.detach().clone() for p in self.model.parameters()]\n",
    "                    for p, sp in zip(self.model.parameters(), sampled_params):\n",
    "                        p.copy_(sp)\n",
    "\n",
    "                    pred = self.model(batch)\n",
    "\n",
    "                    # If pred is valid, store it\n",
    "                    # (If you want to skip NaN predictions, you could check here and skip them)\n",
    "                    predictions.append(pred.detach().cpu())\n",
    "\n",
    "                    # Restore original parameters\n",
    "                    for p, bp in zip(self.model.parameters(), backup_params):\n",
    "                        p.copy_(bp)\n",
    "\n",
    "                # Handle cases with zero or one valid sample\n",
    "                if len(predictions) == 0:\n",
    "                    # No valid samples, return NaNs\n",
    "                    preds_mean = torch.full_like(batch.energy, float('nan'))\n",
    "                    preds_var = torch.full_like(batch.energy, float('nan'))\n",
    "                    return preds_mean, preds_var\n",
    "\n",
    "                if len(predictions) == 1:\n",
    "                    # Only one sample, variance is zero\n",
    "                    preds_mean = predictions[0]\n",
    "                    preds_var = torch.zeros_like(preds_mean)\n",
    "                    return preds_mean, preds_var\n",
    "\n",
    "                # Multiple samples: compute mean and var\n",
    "                predictions_tensor = torch.stack(predictions)\n",
    "                preds_mean = predictions_tensor.mean(dim=0)\n",
    "                preds_var = predictions_tensor.var(dim=0, unbiased=False)\n",
    "                return preds_mean, preds_var\n",
    "\n",
    "            elif self.use_swag and self.num_models_collected > 0:\n",
    "                self.swag_sample(scale=1.0, cov=not self.no_cov_mat)\n",
    "                pred = self.model(batch)\n",
    "                return pred, None\n",
    "            else:\n",
    "                pred = self.model(batch)\n",
    "                return pred, None\n",
    "\n",
    "    \n",
    "    def finalize_laplace(self):\n",
    "        if self.use_laplace:\n",
    "            self.hessian_diagonal = []\n",
    "            print(\"total batches: \", self.total_batches)\n",
    "            for sq_grad in self.accumulated_squared_gradients:\n",
    "                h_diag = (sq_grad / self.total_batches) + self.prior_precision\n",
    "                self.hessian_diagonal.append(h_diag)\n",
    "            self.param_means = [p.detach().clone() for p in self.model.parameters()]\n",
    "            self.accumulated_squared_gradients = None\n",
    "\n",
    "           \n",
    "    def finalize(self):\n",
    "        # Finalize SWA\n",
    "        if self.use_swa:\n",
    "            if not hasattr(self, 'train_loader'):\n",
    "                raise ValueError(\"Train loader not set. Please assign train_loader to the trainer.\")\n",
    "            update_bn(self.train_loader, self.averaged_model)\n",
    "            self.model = self.averaged_model.module  # Use the averaged model for evaluation\n",
    "            print(\"SWA has been finalized and batch norms updated.\")\n",
    "\n",
    "        # Finalize SWAG\n",
    "        if self.use_swag:\n",
    "            # SWAG does not require additional finalization\n",
    "            print(\"SWAG has been finalized.\")\n",
    "\n",
    "        # Finalize Laplace\n",
    "        if self.use_laplace:\n",
    "            self.finalize_laplace()\n",
    "            print(\"Laplace Approximation finalized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHANGE THIS TO QM9 DATA BASE DIRECTORY ###\n",
    "db_path = r\"C:\\Users\\Jonat\\Desktop\\Datasets\\qm9.db\" ### <--- CHANGE THIS TO QM9 DATA BASE DIRECTORY\n",
    "\n",
    "# Define a function to convert an ASE Atoms object and associated properties to AtomsData\n",
    "def ase_to_atomsdata(atoms: Atoms, energy_property: str = \"energy_U0\") -> AtomsData:\n",
    "    \"\"\"\n",
    "    Convert an ASE Atoms object from QM9 and selected property to AtomsData object.\n",
    "    By default, use U0 (at 0K) as target energy if available in db.\n",
    "    \"\"\"\n",
    "    # Extract atomic numbers as node features\n",
    "    atomic_numbers = torch.tensor(atoms.get_atomic_numbers(), dtype=torch.long).view(-1, 1)\n",
    "    # Positions\n",
    "    positions = torch.tensor(atoms.get_positions(), dtype=torch.float32)\n",
    "    \n",
    "    # For QM9, no periodicity:\n",
    "    cell = torch.eye(3, dtype=torch.float32)\n",
    "    pbc = torch.tensor([False, False, False])\n",
    "    \n",
    "    # Extract energy from the ASE Atoms info dictionary\n",
    "    # The QM9 ASE database entries often store a dictionary with properties. \n",
    "    # Check `ase.db` documentation or run `ase db qm9.db --help` to see available keys.\n",
    "    # Common keys: 'energy_U0', 'energy_U', 'gap', etc.\n",
    "    energy = torch.tensor([atoms.info.get(energy_property, 0.0)], dtype=torch.float32)\n",
    "    \n",
    "    # Construct edge_index:\n",
    "    # QM9 does not store bonds directly. You can define a cutoff to determine edges:\n",
    "    # For small molecules, a cutoff of ~1.5Å might capture bonds. Adjust as needed.\n",
    "    cutoff = 1.5\n",
    "    pos = atoms.get_positions()\n",
    "    src_list = []\n",
    "    dst_list = []\n",
    "    num_nodes = pos.shape[0]\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i+1, num_nodes):\n",
    "            dist = ((pos[i] - pos[j])**2).sum()**0.5\n",
    "            if dist < cutoff:\n",
    "                src_list.append(i)\n",
    "                dst_list.append(j)\n",
    "                src_list.append(j)\n",
    "                dst_list.append(i)\n",
    "    edge_index = torch.tensor([src_list, dst_list], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # For QM9, no edge_shift needed:\n",
    "    edge_shift = None\n",
    "    \n",
    "    data = AtomsData(\n",
    "        node_features=atomic_numbers,\n",
    "        edge_index=edge_index,\n",
    "        node_positions=positions,\n",
    "        energy=energy,\n",
    "        cell=cell,\n",
    "        pbc=pbc,\n",
    "        edge_shift=edge_shift\n",
    "    )\n",
    "    data.validate()\n",
    "    return data\n",
    "\n",
    "# Connect to QM9 database\n",
    "with connect(db_path) as db:\n",
    "    data_list = []\n",
    "    # Remove limit or set it to a larger number\n",
    "    for row in db.select():\n",
    "        atoms = row.toatoms()\n",
    "        data_obj = ase_to_atomsdata(atoms, energy_property=\"energy_U0\")\n",
    "        data_list.append(data_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHANGE THIS TO SPLIT DIRECTORY ###\n",
    "split_path = r\"C:\\Users\\Jonat\\Desktop\\Datasets\\randomsplits_110k_10k_rest.json\" ### <--- CHANGE THIS TO SPLIT DIRECTORY\n",
    "\n",
    "with open(split_path, \"r\") as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "train_indices = splits[\"train\"]\n",
    "val_indices = splits[\"validation\"]\n",
    "test_indices = splits[\"test\"]\n",
    "\n",
    "# data_list is your full dataset\n",
    "train_data = [data_list[i] for i in train_indices]\n",
    "val_data = [data_list[i] for i in val_indices]\n",
    "test_data = [data_list[i] for i in test_indices]\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 32  # or any suitable batch size\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_data, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=collate_data, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_data, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = PaiNN(node_size=64, edge_size=20, num_interaction_blocks=2)\n",
    "\n",
    "# Configure Trainer with desired functionality\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    lr=5e-4,\n",
    "    loss_type=\"mse\",\n",
    "    use_swa=False,\n",
    "    use_swag=False,\n",
    "    swa_lrs=1e-4,\n",
    "    swa_start_percent=0.7,\n",
    "    annealing_percent=0.1,\n",
    "    annealing_strategy=\"cos\",\n",
    "    max_num_models=50,\n",
    "    use_sam=True,\n",
    "    use_asam=False,\n",
    "    sam_rho=2e-3,\n",
    "    use_laplace=False,\n",
    "    num_laplace_samples=5,\n",
    "    prior_precision=1.0,\n",
    "    max_steps=100\n",
    ")\n",
    "\n",
    "# Assign the train_loader to the trainer for SWA finalization\n",
    "trainer.train_loader = train_loader\n",
    "\n",
    "# Set max_steps in Trainer to compute SWA parameters\n",
    "trainer.set_max_steps(trainer.max_steps)  # This sets swa_start_step and annealing_steps based on max_steps=1000\n",
    "\n",
    "# Define validation frequency\n",
    "validate_every_steps = 10  # Perform validation every 10 training steps\n",
    "\n",
    "# Initialize training step counters\n",
    "last_validation_step = 0      # Step counter for last validation\n",
    "stop_training = False         # Flag to stop training early\n",
    "epoch = 0                     # Initialize epoch counter\n",
    "\n",
    "# Start training\n",
    "for epoch in itertools.count(epoch):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        loss_val = trainer.train_step(batch)\n",
    "        # Compute MAE\n",
    "        preds = trainer.model(batch)\n",
    "        targets = batch.energy  # Assuming 'energy' is the target\n",
    "        mae = F.l1_loss(preds, targets).item()\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Step {trainer.current_step}, Train MAE: {mae:.4f}\")\n",
    "        \n",
    "        # Check if it's time to validate\n",
    "        if trainer.current_step - last_validation_step >= validate_every_steps:\n",
    "            # Validation Phase\n",
    "            model.eval()\n",
    "            val_maes = []\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    val_preds = trainer.model(val_batch)\n",
    "                    val_targets = val_batch.energy  # Assuming 'energy' is the target\n",
    "                    val_mae = F.l1_loss(val_preds, val_targets).item()\n",
    "                    val_maes.append(val_mae)\n",
    "            avg_val_mae = sum(val_maes) / len(val_maes)\n",
    "            print(f\"Step {trainer.current_step}, Validation MAE: {avg_val_mae:.4f}\")\n",
    "            last_validation_step = trainer.current_step  # Update last validation step\n",
    "        \n",
    "        # Check if maximum training steps are reached using Trainer's max_steps\n",
    "        if trainer.current_step >= trainer.max_steps:\n",
    "            stop_training = True\n",
    "            break\n",
    "    \n",
    "    # End-of-epoch logic (for SWA/SWAG)\n",
    "    trainer.end_epoch()\n",
    "    \n",
    "    if stop_training:\n",
    "        print(\"Reached maximum number of training steps.\")\n",
    "        break\n",
    "\n",
    "\n",
    "# Finalize SWA/SWAG and Laplace if enabled\n",
    "trainer.finalize()\n",
    "\n",
    "# Testing Phase\n",
    "model.eval()\n",
    "test_maes = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        test_preds = model(batch)\n",
    "        test_targets = batch.energy  # Assuming 'energy' is the target\n",
    "        test_mae = F.l1_loss(test_preds, test_targets).item()\n",
    "        test_maes.append(test_mae)\n",
    "avg_test_mae = sum(test_maes) / len(test_maes)\n",
    "print(f\"Test MAE: {avg_test_mae:.4f}\")\n",
    "\n",
    "# Predict with Uncertainty using Laplace or SWAG\n",
    "if trainer.use_laplace:\n",
    "    def get_laplace_predictions(trainer, test_loader, num_samples=10):\n",
    "\n",
    "        trainer.model.eval()\n",
    "        sample_batch = next(iter(test_loader))\n",
    "        predictions = []\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            preds_mean, _ = trainer.predict(sample_batch)  # Sample predictions\n",
    "            predictions.append(preds_mean)\n",
    "        \n",
    "        predictions = torch.stack(predictions)  # [num_samples, batch_size, output_dim]\n",
    "        preds_mean = predictions.mean(dim=0)    # [batch_size, output_dim]\n",
    "        preds_var = predictions.var(dim=0)      # [batch_size, output_dim]\n",
    "\n",
    "        return preds_mean, preds_var\n",
    "    \n",
    "    # Get Laplace ensemble predictions\n",
    "    laplace_mean, laplace_variance = get_laplace_predictions(trainer, test_loader, num_samples=10)\n",
    "\n",
    "    print(\"Laplace Predictions Mean:\", laplace_mean)\n",
    "    print(\"Laplace Predictions Variance:\", laplace_variance)\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "if trainer.use_swag:\n",
    "    \n",
    "    def get_swag_predictions(trainer, test_loader, num_samples=10):\n",
    "        trainer.model.eval()\n",
    "        sample_batch = next(iter(test_loader))\n",
    "        predictions = []\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            pred, _ = trainer.predict(sample_batch)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        predictions = torch.stack(predictions)  # [num_samples, batch_size, output_dim]\n",
    "        preds_mean = predictions.mean(dim=0)    # [batch_size, output_dim]\n",
    "        preds_var = predictions.var(dim=0)      # [batch_size, output_dim]\n",
    "        \n",
    "        return preds_mean, preds_var\n",
    "    \n",
    "    # Get SWAG ensemble predictions\n",
    "    swag_mean, swag_variance = get_swag_predictions(trainer, test_loader, num_samples=10)\n",
    "\n",
    "    print(\"SWAG Predictions Mean:\", swag_mean)\n",
    "    print(\"SWAG Predictions Variance:\", swag_variance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
